{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f1a06ac",
   "metadata": {},
   "source": [
    "\n",
    "# DOLPHIN â€” Quickstart Demo Notebook (NDD Diagnostic)\n",
    "\n",
    "This notebook helps you set up and run the forked repository **`DOLPHIN-NDD_diagnostic`** (fork of SCUT-DLVC/DOLPHIN).  \n",
    "It guides you through environment setup, minimal preprocessing, quick evaluation, and feature/embedding extraction.\n",
    "\n",
    "> **Tested context:** Windows 10/11 + Conda + Python 3.8â€“3.11.  \n",
    "> **Repo:** `https://github.com/RichardLadislav/DOLPHIN-NDD_diagnostic` (fork of SCUTâ€‘DLVCLab/DOLPHIN)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf5b33",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Environment setup\n",
    "\n",
    "You can use either **Conda** or **pip**. The original repo pins Python 3.8.16; it also works with Python 3.10/3.11 if you adjust dependencies (not guaranteed).  \n",
    "If you hit **NumPy 2.0** compatibility errors, prefer **NumPy < 2.0** or rebuild affected wheels.\n",
    "\n",
    "### Option A â€” Conda (recommended on Windows)\n",
    "```bash\n",
    "# Create env (choose one Python version that works for you)\n",
    "conda create -n dolphin python=3.10 -y\n",
    "conda activate dolphin\n",
    "\n",
    "# Install PyTorch (choose your CUDA version; see https://pytorch.org/get-started/locally/)\n",
    "# Example: CUDA 12.x build\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Core deps from requirements; pin numpy<2 to avoid ABI mismatches in some libs\n",
    "pip install -r requirements.txt --no-deps\n",
    "pip install numpy<2 matplotlib termcolor Pillow opencv-python timm pytorch-wavelets\n",
    "```\n",
    "\n",
    "### Option B â€” Pure pip (virtualenv)\n",
    "```bash\n",
    "python -m venv .venv\n",
    "# Windows:\n",
    ".\\.venv\\Scripts\\activate\n",
    "# Linux/macOS:\n",
    "source .venv/bin/activate\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install -r requirements.txt --no-deps\n",
    "pip install numpy<2 matplotlib termcolor Pillow opencv-python timm pytorch-wavelets\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6bad40",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Clone the repository & set working directory\n",
    "\n",
    "> If you've already cloned your fork, just update `REPO_DIR` below to match your path.\n",
    "\n",
    "```bash\n",
    "# Choose a parent folder\n",
    "cd C:\\dev\n",
    "\n",
    "# Clone your fork (replace with your URL if different)\n",
    "git clone https://github.com/RichardLadislav/DOLPHIN-NDD_diagnostic.git\n",
    "cd DOLPHIN-NDD_diagnostic\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50db83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure Python-side path to the repo so you can import modules directly in this notebook.\n",
    "import sys, os, pathlib\n",
    "REPO_DIR = pathlib.Path(r\"C:\\dev\\dolphin_initial_testing\\DOLPHIN\").resolve()  # <- CHANGE to your local path\n",
    "if REPO_DIR.exists():\n",
    "    sys.path.insert(0, str(REPO_DIR))\n",
    "    print(\"Repo path added to sys.path:\", REPO_DIR)\n",
    "else:\n",
    "    print(\"WARNING: REPO_DIR does not exist. Please edit the path above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1877beb",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Quick GPU / Torch check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83deff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502859b7",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Data layout and sanity check\n",
    "\n",
    "Datasets used in the original paper/repo:\n",
    "- **CASIA-OLHWDB2**\n",
    "- **DCOH-E**\n",
    "- **SCUT-COUCH2009**\n",
    "\n",
    "Create the following structure and place the raw data accordingly:\n",
    "```\n",
    "DOLPHIN/\n",
    "â”œâ”€ data-raw/\n",
    "â”‚  â”œâ”€ OLHWDB2/           # raw .txt/.dat files\n",
    "â”‚  â”œâ”€ DCOH_E/\n",
    "â”‚  â””â”€ SCUT_COUCH2009/\n",
    "â””â”€ data/\n",
    "```\n",
    "\n",
    "> If you only want a **tiny dry-run**, put a **very small** subset (e.g., a few writers with a couple of files) into `data-raw/...`.\n",
    "This will speed up preprocessing and avoid large downloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "root = REPO_DIR\n",
    "print(\"Expecting folders under:\", root)\n",
    "\n",
    "print(\"\\nRaw data present?\")\n",
    "for d in [\"data-raw/OLHWDB2\", \"data-raw/DCOH_E\", \"data-raw/COUCH2009\"]:\n",
    "    p = root / d\n",
    "    print(f\"{d:30s}\", \"OK\" if p.exists() else \"MISSING\")\n",
    "\n",
    "print(\"\\nProcessed data folder:\")\n",
    "print((root / \"data\").resolve(), \"exists?\" , (root / \"data\").exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc2c7e",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Preprocess datasets and create the **OLIWER** split\n",
    "\n",
    "Run the repo's preprocessing script(s). You can run from terminal or within this notebook.\n",
    "Large datasets will take time; start with a tiny subset for verification.\n",
    "\n",
    "### CLI (Terminal)\n",
    "```bash\n",
    "# From the repo root (adjust interp if you want to resample the trajectories)\n",
    "python preprocess.py --src_root ./data-raw/OLHWDB2 --interp 4\n",
    "python preprocess.py --src_root ./data-raw/DCOH_E --interp 4\n",
    "python preprocess.py --src_root ./data-raw/SCUT_COUCH2009 --interp 4\n",
    "\n",
    "# Merge and split into OLIWER (train/test)\n",
    "python divide.py --data_root ./data --out_root ./data/OLIWER --train_ratio 0.8 --seed 42\n",
    "\n",
    "# Optional: verify PKLs\n",
    "python verify_pkls.py --root ./data/OLIWER\n",
    "```\n",
    "\n",
    "### From notebook (subprocess)\n",
    "> Uncomment the lines below to run directly here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214cb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import subprocess, sys\n",
    "# cmds = [\n",
    "#     [sys.executable, \"preprocess.py\", \"--src_root\", \"./data-raw/OLHWDB2\", \"--interp\", \"4\"],\n",
    "#     [sys.executable, \"preprocess.py\", \"--src_root\", \"./data-raw/DCOH_E\", \"--interp\", \"4\"],\n",
    "#     [sys.executable, \"preprocess.py\", \"--src_root\", \"./data-raw/SCUT_COUCH2009\", \"--interp\", \"4\"],\n",
    "#     [sys.executable, \"divide.py\", \"--data_root\", \"./data\", \"--out_root\", \"./data/OLIWER\", \"--train_ratio\", \"0.8\", \"--seed\", \"42\"],\n",
    "#     [sys.executable, \"verify_pkls.py\", \"--root\", \"./data/OLIWER\"],\n",
    "# ]\n",
    "# for c in cmds:\n",
    "#     print(\"Running:\", \" \".join(c))\n",
    "#     subprocess.run(c, cwd=str(REPO_DIR))\n",
    "print(\"Preprocessing commands prepared (commented out). Run them in your terminal for full datasets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b9aba",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Load pretrained weights & run a tiny evaluation\n",
    "\n",
    "Place your weights (e.g., `model.pth`) under `./weights/` in the repo.  \n",
    "Then run `test.py` with a tiny gallery/query to confirm everything works.\n",
    "\n",
    "> **Security tip from PyTorch:** If you see a warning about `torch.load(..., weights_only=False)`, prefer `weights_only=True` once your code supports strict state-dict loading.\n",
    "\n",
    "### CLI example\n",
    "```bash\n",
    "python test.py --weights ./weights/model.pth --gallery_root ./data/OLIWER/test-tf.pkl --query_root ./data/OLIWER/test-tf.pkl --topk 5\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: call test.py programmatically (requires processed data & weights).\n",
    "import subprocess, sys, pathlib\n",
    "\n",
    "weights = REPO_DIR / \"weights\" / \"model.pth\"        # <- put your file here\n",
    "gallery = REPO_DIR / \"data\" / \"OLIWER\" / \"test-tf.pkl\"\n",
    "if weights.exists() and gallery.exists():\n",
    "    cmd = [sys.executable, \"test.py\", \"--weights\", str(weights), \"--gallery_root\", str(gallery), \"--query_root\", str(gallery), \"--topk\", \"5\"]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, cwd=str(REPO_DIR))\n",
    "else:\n",
    "    print(\"Weights or gallery PKL missing. Please add them first.\")\n",
    "print(\"Prepared cell for tiny evaluation (commented).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571fae21",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Extract embeddings for custom analysis\n",
    "\n",
    "Below is a minimal example showing how to load the model class and extract embeddings (`x`) and logits (`y`) from the **Head**.\n",
    "Adapt paths to your PKL dataset. For a quick check, use a tiny sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c942d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version 1.0 - simple forward pass on a tiny batch from PKL\n",
    "\n",
    "import pickle, torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from joblib import load\n",
    "# Local imports from the repo\n",
    "try:\n",
    "    from model import DOLPHIN\n",
    "    from dataset import Writing, collate_fn\n",
    "    print(\"Imported repo modules successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Import error:\", e)\n",
    "    print(\"Make sure REPO_DIR is correct and repo is on sys.path.\")\n",
    "\n",
    "# --- Config ---\n",
    "PKL_PATH = REPO_DIR / \"data\" / \"OLIWER\" / \"test-tf.pkl\"   # <- change to a small PKL for a quick check\n",
    "NUM_CLASSES = 1000                                        # placeholder; not used if we only take embeddings\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "if PKL_PATH.exists():\n",
    "   # Load a tiny subset\n",
    "    #data = pickle.load(open(PKL_PATH, \"rb\"))\n",
    "    data = load(open(PKL_PATH, \"rb\"))  # joblib load (faster\n",
    "    # 'data' is typically a dict: {writer_id: [np.array(T, 3), ...]}\n",
    "    # Flatten into list of (writer_id, array)\n",
    "    samples = []\n",
    "    for k, arrs in data.items():\n",
    "        for a in arrs[:2]:   # take at most 2 per writer to keep tiny\n",
    "            samples.append((k, a))\n",
    "            if len(samples) >= 32:\n",
    "                break\n",
    "        if len(samples) >= 32:\n",
    "            break\n",
    "\n",
    "    # Build torch tensors (x, y, p): x=(T,3) ~ [x, y, p], y=label, p=valid_mask (optional)\n",
    "    def to_tensor(sample):\n",
    "        lbl, arr = sample\n",
    "        arr = np.asarray(arr, dtype=np.float32)\n",
    "        x = torch.tensor(arr.T, dtype=torch.float32)  # shape (3, T)\n",
    "        y = torch.tensor(lbl, dtype=torch.long)\n",
    "        p = torch.ones(x.shape[-1], dtype=torch.float32)\n",
    "        return x, y, p\n",
    "\n",
    "    tensor_samples = [to_tensor(s) for s in samples]\n",
    "\n",
    "    # Collate (repo provides a collate_fn)\n",
    "    batch = collate_fn(tensor_samples)\n",
    "    xs, ys, ps, lens = batch  # shapes: [B, 3, T], [B], [B, T], [B]\n",
    "\n",
    "    # Create model (dims per paper/code)\n",
    "    model = DOLPHIN(d_in=3, num_classes=NUM_CLASSES).eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward: returns embedding and logits\n",
    "        emb, logits = model(xs)\n",
    "    print(\"Embeddings shape:\", emb.shape)   # (B, d_hidden) per Head\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "else:\n",
    "    print(\"PKL not found at:\", PKL_PATH)\n",
    "    print(\"Please run preprocessing and divide steps first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf1968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from joblib import load\n",
    "from tqdm import tqdm   # <-- progress bar\n",
    "from model import DOLPHIN\n",
    "from dataset import Writing  # your class\n",
    "\n",
    "# 1) Load your joblib-saved file\n",
    "PKL_PATH = REPO_DIR / \"data\" / \"OLIWER\" / \"test-tf.pkl\"\n",
    "handwriting_info = load(str(PKL_PATH), mmap_mode='r')\n",
    "\n",
    "# 2) Dataset\n",
    "ds = Writing(handwriting_info, transform=None, train=False)\n",
    "num_classes = ds.users_cnt\n",
    "print(f\"Users: {num_classes} | Samples: {len(ds)} | F={ds.feature_dims}\")\n",
    "\n",
    "# 3) Collate -> (xs[B,3,T], ys[B], ps[B,T], lens[B])\n",
    "def collate_fn_dolphin(batch, cols=(0,1,2)):\n",
    "    arrs, lens, labels = zip(*batch)\n",
    "    lens = np.asarray(lens, dtype=np.int64)\n",
    "    B, T_max = len(arrs), int(lens.max())\n",
    "\n",
    "    xs = torch.zeros(B, T_max, len(cols), dtype=torch.float32)  # (B,T,3)\n",
    "    ps = torch.zeros(B, T_max, dtype=torch.float32)\n",
    "    ys = torch.tensor(labels, dtype=torch.long)\n",
    "    lens_t = torch.tensor(lens, dtype=torch.int64)\n",
    "\n",
    "    for i, (a, L) in enumerate(zip(arrs, lens)):\n",
    "        a = torch.as_tensor(a, dtype=torch.float32)     # (T,F)\n",
    "        sel = a[:L, cols]                               # select (x,y,p)\n",
    "        xs[i, :L, :] = sel\n",
    "        ps[i, :L] = 1.0\n",
    "\n",
    "    return xs, ys, ps, lens_t\n",
    "\n",
    "# 4) Small loader for a smoke test\n",
    "idx = list(range(len(ds))); random.shuffle(idx); idx = idx[:64]\n",
    "loader = DataLoader(\n",
    "    Subset(ds, idx),\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    #collate_fn=lambda b: collate_fn_dolphin(b, cols=(0,1,2))\n",
    ")\n",
    "\n",
    "# 5) Model (+ correct call)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = DOLPHIN(d_in=3, num_classes=num_classes).to(device).eval()\n",
    "\n",
    "# --- tqdm progress bar for inference ---\n",
    "embeddings, logits_all = [], []\n",
    "with torch.no_grad():\n",
    "    for xs, ys, ps, lens in tqdm(loader, desc=\"Running DOLPHIN inference\", unit=\"batch\"):\n",
    "        xs = xs.to(device)\n",
    "        lens = lens.to(device)\n",
    "        y_vec, y_prob, f3 = model(xs, lens)\n",
    "        embeddings.append(y_vec.cpu())\n",
    "        logits_all.append(y_prob.cpu())\n",
    "\n",
    "# Combine all batches if needed\n",
    "embeddings = torch.cat(embeddings)\n",
    "logits_all = torch.cat(logits_all)\n",
    "\n",
    "print(f\"\\nDone. Collected {len(embeddings)} embeddings.\")\n",
    "print(\"Embedding shape:\", tuple(embeddings.shape))\n",
    "print(\"Logits shape:\", tuple(logits_all.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd83b1",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Visualize one handwriting trajectory\n",
    "\n",
    "A quick 2D plot of `(x, y)` over time with pressure as line width/alpha.  \n",
    "This helps sanity-check that the trajectories look reasonable after preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dfb5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# ðŸ“Š Visualize a handwriting trajectory (x-y-p)\n",
    "# ================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def plot_trajectory(sample_array, title=\"Trajectory (xâ€“y over time)\"):\n",
    "    \"\"\"\n",
    "    Plot handwriting trajectory with pressure-modulated line width.\n",
    "    sample_array: (T, 3) -> [x, y, p]\n",
    "    \"\"\"\n",
    "    arr = np.asarray(sample_array, dtype=np.float32)\n",
    "    if arr.ndim != 2 or arr.shape[1] < 2:\n",
    "        print(\"Unexpected shape:\", arr.shape)\n",
    "        return\n",
    "    \n",
    "    x, y = arr[:, 0], arr[:, 1]\n",
    "    p = arr[:, 2] if arr.shape[1] >= 3 else np.ones_like(x)\n",
    "\n",
    "    # Normalize pressure (0â€“1)\n",
    "    p_norm = (p - p.min()) / (p.ptp() + 1e-6)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for i in range(1, len(x)):\n",
    "        lw = 0.5 + 3.0 * float(p_norm[i])  # line width based on pressure\n",
    "        plt.plot([x[i - 1], x[i]], [y[i - 1], y[i]], color=\"black\", linewidth=lw)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis(\"equal\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# ================================================\n",
    "# ðŸ§© Choose a sample to visualize\n",
    "# ================================================\n",
    "try:\n",
    "    # You already have 'ds' from: ds = Writing(handwriting_info, transform=None, train=False)\n",
    "    # Pick random or specific index\n",
    "    idx = random.randint(0, len(ds) - 1)\n",
    "    sample, length, label = ds[idx]    # sample: (T, F)\n",
    "    print(f\"Selected sample idx={idx}, label={label}, length={length}\")\n",
    "\n",
    "    # Pick columns corresponding to (x, y, p)\n",
    "    COLS = (0, 1, 2)  # change if time_functions outputs differ (e.g., (x,y,t,p,...))\n",
    "    arr_T3 = sample[:, COLS]\n",
    "\n",
    "    # Plot\n",
    "    plot_trajectory(arr_T3, title=f\"Writer {label} â€“ sample {idx}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")\n",
    "    print(\"Make sure you've already loaded the dataset `ds` and joblib file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589e9d23",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Troubleshooting notes\n",
    "\n",
    "- **NumPy 2.0 ABI warning/error**: If a C-extension was built against NumPy 1.x, pin `numpy<2` or rebuild the package.  \n",
    "- **`torch.load` pickle warning**: Use `weights_only=True` when the code supports strict state dict loading; otherwise ensure the source of the `.pth` is trusted.  \n",
    "- **Qt/PySide event loop warnings**: If you see GUI loop warnings when plotting, try running this notebook with the default Jupyter kernel (no PySide6 integration).  \n",
    "- **OpenCV import errors**: On Windows, prefer `pip install opencv-python` (not `opencv-contrib-python` unless needed).  \n",
    "- **CUDA not available**: The code can run on CPU but will be slow. Verify your NVIDIA driver + CUDA runtime matches the installed PyTorch build.\n",
    "- **Huge PKLs**: Start with a tiny subset to validate the pipeline; once it works, scale up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41232d4d",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Next steps\n",
    "\n",
    "- Swap the **Head** for a clinical/diagnostic head (binary or multi-class) and fine-tune on your dataset.  \n",
    "- Add early/late fusion with image-based ViT on rendered strokes (if exploring multimodal).  \n",
    "- Export embeddings (CSV/NPZ) and test classical ML baselines (LR/SVM/RF) for quick diagnostics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dolphin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
