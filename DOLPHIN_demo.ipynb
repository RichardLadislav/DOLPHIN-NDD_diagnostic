{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f1a06ac",
   "metadata": {},
   "source": [
    "\n",
    "# DOLPHIN — Quickstart Demo Notebook (NDD Diagnostic)\n",
    "\n",
    "This notebook helps you set up and run the forked repository **`DOLPHIN-NDD_diagnostic`** (fork of SCUT-DLVC/DOLPHIN).  \n",
    "It guides you through environment setup, minimal preprocessing, quick evaluation, and feature/embedding extraction.\n",
    "\n",
    "> **Tested context:** Windows 10/11 + Conda + Python 3.8–3.11.  \n",
    "> **Repo:** `https://github.com/RichardLadislav/DOLPHIN-NDD_diagnostic` (fork of SCUT‑DLVCLab/DOLPHIN)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf5b33",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Environment setup\n",
    "\n",
    "You can use either **Conda** or **pip**. The original repo pins Python 3.8.16; it also works with Python 3.10/3.11 if you adjust dependencies (not guaranteed).  \n",
    "If you hit **NumPy 2.0** compatibility errors, prefer **NumPy < 2.0** or rebuild affected wheels.\n",
    "\n",
    "### Option A — Conda (recommended on Windows)\n",
    "```bash\n",
    "# Create env (choose one Python version that works for you)\n",
    "conda create -n dolphin python=3.10 -y\n",
    "conda activate dolphin\n",
    "\n",
    "# Install PyTorch (choose your CUDA version; see https://pytorch.org/get-started/locally/)\n",
    "# Example: CUDA 12.x build\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Core deps from requirements; pin numpy<2 to avoid ABI mismatches in some libs\n",
    "pip install -r requirements.txt --no-deps\n",
    "pip install numpy<2 matplotlib termcolor Pillow opencv-python timm pytorch-wavelets\n",
    "```\n",
    "\n",
    "### Option B — Pure pip (virtualenv)\n",
    "```bash\n",
    "python -m venv .venv\n",
    "# Windows:\n",
    ".\\.venv\\Scripts\\activate\n",
    "# Linux/macOS:\n",
    "source .venv/bin/activate\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install -r requirements.txt --no-deps\n",
    "pip install numpy<2 matplotlib termcolor Pillow opencv-python timm pytorch-wavelets\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6bad40",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Clone the repository & set working directory\n",
    "\n",
    "> If you've already cloned your fork, just update `REPO_DIR` below to match your path.\n",
    "\n",
    "```bash\n",
    "# Choose a parent folder\n",
    "cd C:\\dev\n",
    "\n",
    "# Clone your fork (replace with your URL if different)\n",
    "git clone https://github.com/RichardLadislav/DOLPHIN-NDD_diagnostic.git\n",
    "cd DOLPHIN-NDD_diagnostic\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50db83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo path added to sys.path: C:\\dev\\dolphin_initial_testing\\DOLPHIN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configure Python-side path to the repo so you can import modules directly in this notebook.\n",
    "import sys, os, pathlib\n",
    "REPO_DIR = pathlib.Path(r\"C:\\dev\\dolphin_initial_testing\\DOLPHIN\").resolve()  # <- CHANGE to your local path\n",
    "if REPO_DIR.exists():\n",
    "    sys.path.insert(0, str(REPO_DIR))\n",
    "    print(\"Repo path added to sys.path:\", REPO_DIR)\n",
    "else:\n",
    "    print(\"WARNING: REPO_DIR does not exist. Please edit the path above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1877beb",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Quick GPU / Torch check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83deff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502859b7",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Data layout and sanity check\n",
    "\n",
    "Datasets used in the original paper/repo:\n",
    "- **CASIA-OLHWDB2**\n",
    "- **DCOH-E**\n",
    "- **SCUT-COUCH2009**\n",
    "\n",
    "Create the following structure and place the raw data accordingly:\n",
    "```\n",
    "DOLPHIN/\n",
    "├─ data-raw/\n",
    "│  ├─ OLHWDB2/           # raw .txt/.dat files\n",
    "│  ├─ DCOH_E/\n",
    "│  └─ SCUT_COUCH2009/\n",
    "└─ data/\n",
    "```\n",
    "\n",
    "> If you only want a **tiny dry-run**, put a **very small** subset (e.g., a few writers with a couple of files) into `data-raw/...`.\n",
    "This will speed up preprocessing and avoid large downloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf4b73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting folders under: C:\\dev\\dolphin_initial_testing\\DOLPHIN\n",
      "\n",
      "Raw data present?\n",
      "data-raw/OLHWDB2               OK\n",
      "data-raw/DCOH_E                MISSING\n",
      "data-raw/COUCH2009             MISSING\n",
      "\n",
      "Processed data folder:\n",
      "C:\\dev\\dolphin_initial_testing\\DOLPHIN\\data exists? True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "root = REPO_DIR\n",
    "print(\"Expecting folders under:\", root)\n",
    "\n",
    "print(\"\\nRaw data present?\")\n",
    "for d in [\"data-raw/OLHWDB2\", \"data-raw/DCOH_E\", \"data-raw/COUCH2009\"]:\n",
    "    p = root / d\n",
    "    print(f\"{d:30s}\", \"OK\" if p.exists() else \"MISSING\")\n",
    "\n",
    "print(\"\\nProcessed data folder:\")\n",
    "print((root / \"data\").resolve(), \"exists?\" , (root / \"data\").exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc2c7e",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Preprocess datasets and create the **OLIWER** split\n",
    "\n",
    "Run the repo's preprocessing script(s). You can run from terminal or within this notebook.\n",
    "Large datasets will take time; start with a tiny subset for verification.\n",
    "\n",
    "### CLI (Terminal)\n",
    "```bash\n",
    "# From the repo root (adjust interp if you want to resample the trajectories)\n",
    "python preprocess.py --src_root ./data-raw/OLHWDB2 --interp 4\n",
    "python preprocess.py --src_root ./data-raw/DCOH_E --interp 4\n",
    "python preprocess.py --src_root ./data-raw/SCUT_COUCH2009 --interp 4\n",
    "\n",
    "# Merge and split into OLIWER (train/test)\n",
    "python divide.py --data_root ./data --out_root ./data/OLIWER --train_ratio 0.8 --seed 42\n",
    "\n",
    "# Optional: verify PKLs\n",
    "python verify_pkls.py --root ./data/OLIWER\n",
    "```\n",
    "\n",
    "### From notebook (subprocess)\n",
    "> Uncomment the lines below to run directly here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214cb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import subprocess, sys\n",
    "# cmds = [\n",
    "#     [sys.executable, \"preprocess.py\", \"--src_root\", \"./data-raw/OLHWDB2\", \"--interp\", \"4\"],\n",
    "#     [sys.executable, \"preprocess.py\", \"--src_root\", \"./data-raw/DCOH_E\", \"--interp\", \"4\"],\n",
    "#     [sys.executable, \"preprocess.py\", \"--src_root\", \"./data-raw/SCUT_COUCH2009\", \"--interp\", \"4\"],\n",
    "#     [sys.executable, \"divide.py\", \"--data_root\", \"./data\", \"--out_root\", \"./data/OLIWER\", \"--train_ratio\", \"0.8\", \"--seed\", \"42\"],\n",
    "#     [sys.executable, \"verify_pkls.py\", \"--root\", \"./data/OLIWER\"],\n",
    "# ]\n",
    "# for c in cmds:\n",
    "#     print(\"Running:\", \" \".join(c))\n",
    "#     subprocess.run(c, cwd=str(REPO_DIR))\n",
    "print(\"Preprocessing commands prepared (commented out). Run them in your terminal for full datasets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b9aba",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Load pretrained weights & run a tiny evaluation\n",
    "\n",
    "Place your weights (e.g., `model.pth`) under `./weights/` in the repo.  \n",
    "Then run `test.py` with a tiny gallery/query to confirm everything works.\n",
    "\n",
    "> **Security tip from PyTorch:** If you see a warning about `torch.load(..., weights_only=False)`, prefer `weights_only=True` once your code supports strict state-dict loading.\n",
    "\n",
    "### CLI example\n",
    "```bash\n",
    "python test.py --weights ./weights/model.pth --gallery_root ./data/OLIWER/test-tf.pkl --query_root ./data/OLIWER/test-tf.pkl --topk 5\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0143a8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: c:\\Users\\tenNovy\\miniconda3\\envs\\dolphin\\python.exe test.py --weights C:\\dev\\dolphin_initial_testing\\DOLPHIN\\weights\\model.pth --gallery_root C:\\dev\\dolphin_initial_testing\\DOLPHIN\\data\\OLIWER\\test-tf.pkl --query_root C:\\dev\\dolphin_initial_testing\\DOLPHIN\\data\\OLIWER\\test-tf.pkl --topk 5\n",
      "Prepared cell for tiny evaluation (commented).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: call test.py programmatically (requires processed data & weights).\n",
    "import subprocess, sys, pathlib\n",
    "\n",
    "weights = REPO_DIR / \"weights\" / \"model.pth\"        # <- put your file here\n",
    "gallery = REPO_DIR / \"data\" / \"OLIWER\" / \"test-tf.pkl\"\n",
    "if weights.exists() and gallery.exists():\n",
    "    cmd = [sys.executable, \"test.py\", \"--weights\", str(weights), \"--gallery_root\", str(gallery), \"--query_root\", str(gallery), \"--topk\", \"5\"]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, cwd=str(REPO_DIR))\n",
    "else:\n",
    "    print(\"Weights or gallery PKL missing. Please add them first.\")\n",
    "print(\"Prepared cell for tiny evaluation (commented).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571fae21",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Extract embeddings for custom analysis\n",
    "\n",
    "Below is a minimal example showing how to load the model class and extract embeddings (`x`) and logits (`y`) from the **Head**.\n",
    "Adapt paths to your PKL dataset. For a quick check, use a tiny sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7c942d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported repo modules successfully.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'couch41'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m     p = torch.ones(x.shape[-\u001b[32m1\u001b[39m], dtype=torch.float32)\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x, y, p\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m tensor_samples = \u001b[43m[\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Collate (repo provides a collate_fn)\u001b[39;00m\n\u001b[32m     46\u001b[39m batch = collate_fn(tensor_samples)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     40\u001b[39m     p = torch.ones(x.shape[-\u001b[32m1\u001b[39m], dtype=torch.float32)\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x, y, p\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m tensor_samples = [\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Collate (repo provides a collate_fn)\u001b[39;00m\n\u001b[32m     46\u001b[39m batch = collate_fn(tensor_samples)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(sample)\u001b[39m\n\u001b[32m     37\u001b[39m arr = np.asarray(arr, dtype=np.float32)\n\u001b[32m     38\u001b[39m x = torch.tensor(arr.T, dtype=torch.float32)  \u001b[38;5;66;03m# shape (3, T)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m y = torch.tensor(\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlbl\u001b[49m\u001b[43m)\u001b[49m, dtype=torch.long)\n\u001b[32m     40\u001b[39m p = torch.ones(x.shape[-\u001b[32m1\u001b[39m], dtype=torch.float32)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x, y, p\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: 'couch41'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle, torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from joblib import load\n",
    "# Local imports from the repo\n",
    "try:\n",
    "    from model import DOLPHIN\n",
    "    from dataset import Writing, collate_fn\n",
    "    print(\"Imported repo modules successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Import error:\", e)\n",
    "    print(\"Make sure REPO_DIR is correct and repo is on sys.path.\")\n",
    "\n",
    "# --- Config ---\n",
    "PKL_PATH = REPO_DIR / \"data\" / \"OLIWER\" / \"test-tf.pkl\"   # <- change to a small PKL for a quick check\n",
    "NUM_CLASSES = 1000                                        # placeholder; not used if we only take embeddings\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "if PKL_PATH.exists():\n",
    "   # Load a tiny subset\n",
    "    #data = pickle.load(open(PKL_PATH, \"rb\"))\n",
    "    data = load(open(PKL_PATH, \"rb\"))  # joblib load (faster\n",
    "    # 'data' is typically a dict: {writer_id: [np.array(T, 3), ...]}\n",
    "    # Flatten into list of (writer_id, array)\n",
    "    samples = []\n",
    "    for k, arrs in data.items():\n",
    "        for a in arrs[:2]:   # take at most 2 per writer to keep tiny\n",
    "            samples.append((k, a))\n",
    "            if len(samples) >= 32:\n",
    "                break\n",
    "        if len(samples) >= 32:\n",
    "            break\n",
    "\n",
    "    # Build torch tensors (x, y, p): x=(T,3) ~ [x, y, p], y=label, p=valid_mask (optional)\n",
    "    def to_tensor(sample):\n",
    "        lbl, arr = sample\n",
    "        arr = np.asarray(arr, dtype=np.float32)\n",
    "        x = torch.tensor(arr.T, dtype=torch.float32)  # shape (3, T)\n",
    "        y = torch.tensor(int(lbl), dtype=torch.long)\n",
    "        p = torch.ones(x.shape[-1], dtype=torch.float32)\n",
    "        return x, y, p\n",
    "\n",
    "    tensor_samples = [to_tensor(s) for s in samples]\n",
    "\n",
    "    # Collate (repo provides a collate_fn)\n",
    "    batch = collate_fn(tensor_samples)\n",
    "    xs, ys, ps, lens = batch  # shapes: [B, 3, T], [B], [B, T], [B]\n",
    "\n",
    "    # Create model (dims per paper/code)\n",
    "    model = DOLPHIN(d_in=3, num_classes=NUM_CLASSES).eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward: returns embedding and logits\n",
    "        emb, logits = model(xs)\n",
    "    print(\"Embeddings shape:\", emb.shape)   # (B, d_hidden) per Head\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "else:\n",
    "    print(\"PKL not found at:\", PKL_PATH)\n",
    "    print(\"Please run preprocessing and divide steps first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd83b1",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Visualize one handwriting trajectory\n",
    "\n",
    "A quick 2D plot of `(x, y)` over time with pressure as line width/alpha.  \n",
    "This helps sanity-check that the trajectories look reasonable after preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dfb5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_trajectory(sample_array, title=\"Trajectory (x-y over time)\"):\n",
    "    # sample_array: shape (T, 3) -> [x, y, p]\n",
    "    arr = np.asarray(sample_array, dtype=np.float32)\n",
    "    if arr.ndim != 2 or arr.shape[1] < 2:\n",
    "        print(\"Unexpected shape:\", arr.shape)\n",
    "        return\n",
    "    x, y = arr[:, 0], arr[:, 1]\n",
    "    p = arr[:, 2] if arr.shape[1] >= 3 else np.ones_like(x)\n",
    "\n",
    "    # Normalize pressure for linewidth\n",
    "    p_norm = (p - p.min()) / (p.ptp() + 1e-6)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for i in range(1, len(x)):\n",
    "        lw = 0.5 + 3.0 * float(p_norm[i])\n",
    "        plt.plot([x[i-1], x[i]], [y[i-1], y[i]], linewidth=lw)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "\n",
    "# Try plotting the first sample from the small list above (if available)\n",
    "try:\n",
    "    if 'samples' in globals() and len(samples) > 0:\n",
    "        plot_trajectory(samples[0][1], title=\"Sample 0: x-y with pressure width\")\n",
    "    else:\n",
    "        print(\"No samples available yet. Run the preprocessing/PKL steps first.\")\n",
    "except Exception as e:\n",
    "    print(\"Plotting error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589e9d23",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Troubleshooting notes\n",
    "\n",
    "- **NumPy 2.0 ABI warning/error**: If a C-extension was built against NumPy 1.x, pin `numpy<2` or rebuild the package.  \n",
    "- **`torch.load` pickle warning**: Use `weights_only=True` when the code supports strict state dict loading; otherwise ensure the source of the `.pth` is trusted.  \n",
    "- **Qt/PySide event loop warnings**: If you see GUI loop warnings when plotting, try running this notebook with the default Jupyter kernel (no PySide6 integration).  \n",
    "- **OpenCV import errors**: On Windows, prefer `pip install opencv-python` (not `opencv-contrib-python` unless needed).  \n",
    "- **CUDA not available**: The code can run on CPU but will be slow. Verify your NVIDIA driver + CUDA runtime matches the installed PyTorch build.\n",
    "- **Huge PKLs**: Start with a tiny subset to validate the pipeline; once it works, scale up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41232d4d",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Next steps\n",
    "\n",
    "- Swap the **Head** for a clinical/diagnostic head (binary or multi-class) and fine-tune on your dataset.  \n",
    "- Add early/late fusion with image-based ViT on rendered strokes (if exploring multimodal).  \n",
    "- Export embeddings (CSV/NPZ) and test classical ML baselines (LR/SVM/RF) for quick diagnostics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dolphin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
